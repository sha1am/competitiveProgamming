#Deep learning :

-----------------------------------------------------------
Start
-----------------------------------------------------------
_____________________________________________________

- Backpropagation through time
	- cross entropy formula
	
_____________________________________________________

- Backpropagation through time for RNN ( Recurrent neural Network) ( used by alexa , google)
- Normal ANN and CNN will not work.
	- because the input is not dependent on previous input/ output.
	- We use RNN
		- where sequence of words/input is important.
		
(brackey);
	- types of structures
		- one to one
		- many to one etc. (words to next word)
		- one to many - ( image ---> caption)
		- 
	- Context data?
_____________________________________________________

# Deep learning after minors ( RNN - Recurrent Neural network.)
	- ann, backpropagation, 
	- Inputs are independent of each other ( till now)
		- 1 st row is not dependedent on second row
		- CNN - 	
			- Images pr convultion.
			- generally used for images
	- texual data  next word prediction.
		- lemmentization. 
		- stock market price is similar problem. 
			- we are predicting the next days stock.
	- 3rd unit will be dealing 	
	- recurrent relation like fiboonaci number.
	- till now there was no relation between input to output.
		- but in sentences , there will be dependency of previous .
	- Image captioning , using RNN.
	
_____________________________________________________

Self Notes :-
- Perceptron Model
		- perceptron model used for classification of linear models.
		- It consists of input/weight and bais.
		- Single perceptron model	
			- Artitecture of single layer perceptron?
			- formula of single layer perceptron. 
				- y= (sigma wi*xi) +b; 
		- Algo for single layer perceptron (confusion)
			- Initialize w=0 and b=0 (ie.bais) ,Set Alpha (ie. learning rate) anything between 0 -1.
			
			- while Stopping criteria is false. Iterate below steps
			- 
		

_____________________________________________________

- Transfer learning 
	- Purpose of transfer learning?
		- we generally use pretrained model and we modify the model existing
		- Similar problems only , transfer learning.
		
	- 
	- 
_____________________________________________________

- COnvulation
	- (input - filter + 2*padding / stride) /2   + 1;
	- 
	
- Max pooling		
			-
- image -> convultioin -> relu -> pooling 
		- CRP+
		
- this will convert the matrix into smaller size 
	- and will convert the matrix into 1D array and then it mmatches with the new images. 
	
- Nummerical and backpropagation
	- neural network , activation function , single neutron
	- dropout
	- early stopping
	- momentum based gradium
	- adamm based optimizer. 
	- Weight bais. 
	- how weight are adjusted fromm the beggining till end.
	- All activation functions
	- activation functions maths behind it 
_____________________________________________________

- What is the need of convulational network?
- Convulation neural network 
	- convulation and stride.
	- padding 
	- pooling
	- flattening 
	- classify

- 
_____________________________________________________

- convulation using Cnn
	- How to reduce imae size using sliding square. 
	- 6x6 ---> 
	
-
_____________________________________________________

- Backpropagate
	- kuch samajh nhi aa rha.
	- s


_____________________________________________________

- leaky value - ( negative values or something)
- Multiclass 
	- two types of out put.
- Difference between Non -Exclusive and Mutually Exclusive.
	- 1 data point can belong to more than one class in non exclusive multiclass classification.
	- 1 data point can't belong to only one class in non exclusive multiclass classification.
- softmax function
	- estimates values beween -1 and 1.
	
- Capital X and Capital W  means array or matrix
- quadratic cost function
- 



_____________________________________________________

- concept of rotation (or notation)
- cost function (log loss) ( cross entropy function ig)
	- summmation from 1 to N y*log(y) + ( 1-y)log(1-y)
- activation funtion 
	-
- Hidden layer 
	- 2> hidden layer can be called a deep neural network.
- the output and in put of the inner layer are variables.
	-we need to change the cost function of the , to fix this.
- universal approximation
	- continuosly function can be handled by neural network.
	- if in chunks or not continuos , then it will not approximate.
- add continuity 
	- they are diffrentiable
- if we want output in the range of 0-1 , then sigmoid
- if we want output in the range of -1 -1 , then tanh function.( ie.Hyperbolic tangent function)
- Relu( rectified linear unit)
	- deals with vanishing gradient.
- gradient 
	- rate of change , 
- difference between perceptron and neuron
	- 
- stocastic gradient method	
	- one - one 
- mini batch 
	- data is divided into small chunks (generally batch of 32)
		- it will not need all data at once
		
- softback function , backpropagation

_____________________________________________________
- Sigmoid function is 1/1+e^x    
	- (x is something w1x1 + w2x2 +....) 
	- range 0/1

- Tanh function
	- (e^x -e^-x)/(e^x +e^-x)
	
- Need to add activation function to add multilinearity.
- How find this derivative ( convergence)
- random.seed(0) 
	- same values generate kr  skte hai 
- gradient descent for optimnal w and m. (ig)
_________________________________________________________________
- Biological Neuron
	- 
- Artificial Neuron
- Mc cullah Pitts Model
	- input 0/1
	- output 0/1
		- 
	- xor cannot be implementated in this model.(with > 2 inputs)
	- single perceptiron is linear classifier 
		- which can give out put in 0/1.
	- number of input (2- line, 3-plane, >3 hyperplane)
- Single Perception Model
	- input ( can be real)
	- add weights
	- output binary (0/1)
	- Sigma wi xi > T
	- find weights such that error minimum
		- keep on learning the weight till the equation matches the equation.
	- y = mx + c
		-
	- w is perpendicular vector to that line.

- multilayer Perception 
	- activation function
		
			
-----------------------------------------------------------
Code 
-----------------------------------------------------------
- model.add(Dense(noOfNeurons,input_dim=1,activation='relu'))
	- eg model.add(Dense(4,input_dim=1,activation='linear'))
- epochs : Number of iterations.
	- model.fit(x,y,epochs=500, verbose=1)



#resources 
- https://keras.io/examples/vision/captcha_ocr/
-----------------------------------------------------------
Doubts
-----------------------------------------------------------
- If NN run twice will it give same results?
	-

-----------------------------------------------------------
END
-----------------------------------------------------------

What if I search ,w hat is not a dog.
kaggle?
crowd analytics
end to end machine learning